{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import ssl\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import fitz\n",
    "import string\n",
    "import sys\n",
    "import json\n",
    "import plotly.io as pio\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/stan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/stan/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../esg_reports/2022_Apple_ESG_Report.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(text):\n",
    "    # remove non-breaking spaces\n",
    "    text = text.replace(u'\\xa0', u' ')\n",
    "    # remove bullet points\n",
    "    text = text.replace(u'•', u'')\n",
    "    # remove any non-alphanumeric, non-hyphen characters\n",
    "    text = re.sub(r'[^A-Za-z0-9- ]', '', text)\n",
    "    # remove words with hyphens, as they could be compound words\n",
    "    text = re.sub(r'\\w+-\\w+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text_str):\n",
    "    # handle punctuation and special characters\n",
    "    text_str = re.sub(r'[^\\w\\s]', '', text_str)\n",
    "    if '-' in text_str:\n",
    "        text_str = text_str.replace(\n",
    "            '- ', '').replace(' -', '').replace(' ,', ',').replace(' .', '.')\n",
    "    text_str = text_str.strip()\n",
    "    return text_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = []\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fitz.open(file_path) as doc:\n",
    "    for page in doc:\n",
    "        text = page.get_text()\n",
    "        # text = cleanup_text(text)  # Apply cleanup_text here\n",
    "        text = text.replace('•', ' ')\n",
    "        text = text_process(text)\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = text.replace('\\u2003', ' ')\n",
    "        text = text.strip()\n",
    "        all_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "processed_text = []\n",
    "\n",
    "for doc in all_text:\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    lemmatized = [lemmatizer.lemmatize(\n",
    "        token) for token in tokens if token not in string.punctuation]\n",
    "    no_stops = [token for token in lemmatized if token not in stop_words]\n",
    "    cleaned_doc = \" \".join(no_stops)\n",
    "    # remove digits from each document\n",
    "    cleaned_doc = re.sub(r'\\d+', '', cleaned_doc)\n",
    "    # remove extra spaces from each document\n",
    "    cleaned_doc = re.sub(' +', ' ', cleaned_doc)\n",
    "    processed_text.append(cleaned_doc)\n",
    "\n",
    "doc_list = [doc.split() for doc in processed_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VADER\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply VADER to each sentence and store the scores\n",
    "sentiment_scores = []\n",
    "for sentence in processed_text:\n",
    "    scores = sid.polarity_scores(sentence)\n",
    "    sentiment_scores.append(scores)\n",
    "\n",
    "# Get individual sentiment scores over the course of the report\n",
    "overall_sentiment = {\n",
    "    'neg': [score['neg'] for score in sentiment_scores],\n",
    "    'neu': [score['neu'] for score in sentiment_scores],\n",
    "    'pos': [score['pos'] for score in sentiment_scores],\n",
    "    'compound': [score['compound'] for score in sentiment_scores]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"neg\": [0.0, 0.0, 0.005, 0.035, 0.039, 0.015, 0.02, 0.06, 0.064, 0.053, 0.039, 0.0, 0.022, 0.038, 0.127, 0.006, 0.032, 0.037, 0.013, 0.12, 0.028, 0.0, 0.024, 0.011, 0.019, 0.005, 0.036, 0.051, 0.046, 0.072, 0.035, 0.019, 0.101, 0.156, 0.01, 0.0, 0.026, 0.027, 0.031, 0.028, 0.114, 0.085, 0.007, 0.045, 0.008, 0.023, 0.0, 0.031, 0.034, 0.032, 0.052, 0.022, 0.005, 0.004, 0.007, 0.022, 0.038, 0.0, 0.026, 0.059, 0.005, 0.004, 0.007, 0.011, 0.066, 0.009, 0.099, 0.005, 0.0, 0.076, 0.029, 0.049, 0.0, 0.037, 0.047, 0.066, 0.017, 0.049, 0.008, 0.122, 0.0, 0.0, 0.014, 0.053, 0.05], \"neu\": [1.0, 0.697, 0.619, 0.78, 0.761, 0.74, 0.659, 0.831, 0.691, 0.742, 0.696, 0.875, 0.68, 0.705, 0.668, 0.808, 0.762, 0.75, 0.822, 0.688, 0.685, 0.678, 0.622, 0.83, 0.786, 0.69, 0.649, 0.683, 0.686, 0.73, 0.755, 0.769, 0.71, 0.641, 0.802, 0.729, 0.766, 0.79, 0.764, 0.73, 0.723, 0.759, 0.69, 0.872, 0.764, 0.762, 0.878, 0.718, 0.762, 0.747, 0.73, 0.82, 0.681, 0.688, 0.725, 0.742, 0.713, 0.707, 0.666, 0.606, 0.766, 0.75, 0.81, 0.808, 0.714, 0.731, 0.616, 0.817, 1.0, 0.732, 0.794, 0.802, 0.721, 0.679, 0.606, 0.676, 0.72, 0.852, 0.726, 0.825, 1.0, 0.968, 0.84, 0.844, 0.87], \"pos\": [0.0, 0.303, 0.377, 0.185, 0.201, 0.244, 0.321, 0.11, 0.245, 0.204, 0.265, 0.125, 0.298, 0.258, 0.205, 0.186, 0.206, 0.213, 0.165, 0.193, 0.287, 0.322, 0.354, 0.158, 0.196, 0.305, 0.314, 0.265, 0.268, 0.199, 0.209, 0.213, 0.189, 0.203, 0.188, 0.271, 0.209, 0.183, 0.205, 0.242, 0.163, 0.156, 0.303, 0.083, 0.228, 0.214, 0.122, 0.25, 0.204, 0.221, 0.218, 0.157, 0.314, 0.308, 0.268, 0.236, 0.248, 0.293, 0.309, 0.335, 0.229, 0.246, 0.183, 0.181, 0.22, 0.26, 0.285, 0.178, 0.0, 0.192, 0.177, 0.149, 0.279, 0.284, 0.347, 0.258, 0.263, 0.099, 0.266, 0.053, 0.0, 0.032, 0.145, 0.103, 0.08], \"compound\": [0.0, 0.9931, 0.9977, 0.9915, 0.9965, 0.9927, 0.9983, 0.7845, 0.9959, 0.9934, 0.9928, 0.4588, 0.9982, 0.9979, 0.9584, 0.9958, 0.994, 0.9968, 0.9952, 0.9806, 0.9984, 0.886, 0.999, 0.9947, 0.9963, 0.9991, 0.9982, 0.999, 0.9987, 0.9972, 0.9951, 0.997, 0.9945, 0.8957, 0.9876, 0.8555, 0.9955, 0.9943, 0.9924, 0.9979, 0.9584, 0.9952, 0.9987, 0.8885, 0.9978, 0.9966, 0.4939, 0.9968, 0.9977, 0.9988, 0.9978, 0.9963, 0.9993, 0.9993, 0.9993, 0.9988, 0.9987, 0.8689, 0.9986, 0.9992, 0.9975, 0.9967, 0.9964, 0.995, 0.9964, 0.9988, 0.9971, 0.996, 0.0, 0.991, 0.997, 0.9847, 0.8519, 0.9927, 0.9982, 0.9978, 0.9992, 0.9442, 0.9962, -0.9316, 0.0, 0.5256, 0.9545, 0.9571, 0.9287]}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(overall_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
