{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import argparse\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import pyLDAvis.gensim_models\n",
    "import ssl\n",
    "import os\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def text_process(text_str):\n",
    "    # handle punctuation and special characters\n",
    "    text_str = re.sub(r'[^\\w\\s]', '', text_str)\n",
    "    if '-' in text_str:\n",
    "        text_str = text_str.replace(\n",
    "            '- ', '').replace(' -', '').replace(' ,', ',').replace(' .', '.')\n",
    "    text_str = text_str.strip()\n",
    "    return text_str\n",
    "\n",
    "\n",
    "all_text = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "with fitz.open(file_path) as doc:\n",
    "    for page in doc:\n",
    "        text = page.get_text()\n",
    "        text = text.replace('â€¢', ' ')\n",
    "        text = text_process(text)\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = text.replace('\\u2003', ' ')\n",
    "        text = text.strip()\n",
    "        all_text.append(text)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "processed_text = []\n",
    "\n",
    "for doc in all_text:\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    lemmatized = [lemmatizer.lemmatize(\n",
    "        token) for token in tokens if token not in string.punctuation]\n",
    "    no_stops = [token for token in lemmatized if token not in stop_words]\n",
    "    cleaned_doc = \" \".join(no_stops)\n",
    "    # remove digits from each document\n",
    "    cleaned_doc = re.sub(r'\\d+', '', cleaned_doc)\n",
    "    # remove extra spaces from each document\n",
    "    cleaned_doc = re.sub(' +', ' ', cleaned_doc)\n",
    "    processed_text.append(cleaned_doc)\n",
    "\n",
    "doc_list = [doc.split() for doc in processed_text]\n",
    "\n",
    "# 1. Convert into list of tokens\n",
    "doc_list = [doc.split() for doc in processed_text]\n",
    "\n",
    "# 2. Create the Dictionary and Corpus\n",
    "dictionary = corpora.Dictionary(doc_list)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in doc_list]\n",
    "\n",
    "# 3. Build the LDA model\n",
    "lda_model = LdaModel(corpus=corpus,  id2word=dictionary,\n",
    "                     passes=10, random_state=1)\n",
    "\n",
    "# 4. Visualize the topics\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis_html = pyLDAvis.prepared_data_to_html(vis)\n",
    "\n",
    "with open(os.path.join(\"topics_data\", os.path.splitext(os.path.basename(file_path))[0] + \".html\"), \"w\") as f:\n",
    "    f.write(pyLDAvis_html)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
